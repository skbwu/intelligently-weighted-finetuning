{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Of2OkJkyMG7s"
   },
   "outputs": [],
   "source": [
    "! pip install accelerate==1.7.0 bitsandbytes==0.46.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYh5-imCBFMf"
   },
   "outputs": [],
   "source": [
    "import sys, os, gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import CLRL, compute_acc, compute_mrpo_objective, compute_mdpo_objective, compute_dpo_objective_many_refs\n",
    "\n",
    "# Mount Google Drive to get access to data and save logs/models\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "\n",
    "# directoriy where data (preference datasets + precomputed log probs)\n",
    "DATA_DIR = \"/content/drive/MyDrive/mypartcs329h/cleaned/\"\n",
    "# directory to save logs\n",
    "LOGS_DIR = \"/content/drive/MyDrive/mypartcs329h/logs/\"\n",
    "# directory to save models\n",
    "MODEL_DIR = \"/content/drive/MyDrive/mypartcs329h/models/\"\n",
    "\n",
    "# precomputed aliases for datasets\n",
    "PRECOMPUTED_ALIASES = {\n",
    "    \"ultrafeedback_binarized\" : \"UltraFeedback\",\n",
    "    \"PKU-SafeRLHF-30K-standard\" : \"SafeRLHF\"\n",
    "}\n",
    "\n",
    "# reference models in MRPO/MDPO\n",
    "REFERENCE_MODELS = [\n",
    "    \"01-ai_Yi-1.5-9B-Chat\",\n",
    "    \"meta-llama_Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"microsoft_Phi-3-medium-128k-instruct\",\n",
    "    \"mistralai_Mistral-7B-Instruct-v0.3\",\n",
    "    \"Qwen_Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen_Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen_Qwen3-4B-Instruct-2507\"\n",
    "]\n",
    "\n",
    "# seeds for experiments - corresponds to different data shuffles\n",
    "SEEDS = [0,1,2, 3, 4]\n",
    "\n",
    "# different alpha methods implemented\n",
    "ALPHA_METHODS = [\"offline_1\", \"offline_2\", \"online_1\",\"arwc_original\", \"arwc_normalized\"]\n",
    "\n",
    "# default base model path ie \\pi_{\\theta}\n",
    "BASE_MODEL_PATH = \"Qwen/Qwen2.5-0.5B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I88f7cHXKFSx"
   },
   "outputs": [],
   "source": [
    "DATASET = \"ultrafeedback_binarized\"\n",
    "SEED = 2\n",
    "USE_MRPO_OVER_MDPO = True\n",
    "ALPHA_METHOD = \"online_1\"\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "assert ALPHA_METHOD in ALPHA_METHODS, f\"ALPHA_METHOD {ALPHA_METHOD} not in {ALPHA_METHODS}\"\n",
    "assert DATASET in PRECOMPUTED_ALIASES.keys(), f\"DATASET {DATASET} not in {list(PRECOMPUTED_ALIASES.keys())}\"\n",
    "assert SEED in SEEDS, f\"SEED {SEED} not in {SEEDS}\"\n",
    "\n",
    "# check that MODEL_DIR has 7 folders each named by REFERENCE_MODELS\n",
    "entries = [d for d in os.listdir(MODEL_DIR) if os.path.isdir(os.path.join(MODEL_DIR, d))]\n",
    "missing = set(REFERENCE_MODELS) - set(entries)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing : {missing}\")\n",
    "\n",
    "# These values are fixed\n",
    "use_4bit = True\n",
    "use_lora = True\n",
    "LR, BETA, NUM_EPOCHS = 1e-4, 0.1, 1\n",
    "BATCH_SIZE = 50 if DATASET == \"PKU-SafeRLHF-30K-standard\" else 25\n",
    "# compute how many batches we will need\n",
    "NUM_BATCHES = (5000 // BATCH_SIZE) if (5000 % BATCH_SIZE) == 0 else (5000 // BATCH_SIZE) + 1\n",
    "use_length_normalization = True\n",
    "compute_objective = compute_mrpo_objective if USE_MRPO_OVER_MDPO else compute_mdpo_objective\n",
    "\n",
    "# preference data (prompt, chosen, rejected)\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/data/train.csv\")\n",
    "val_df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/data/val.csv\")\n",
    "test_df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/data/test.csv\")\n",
    "val_df_prompts = list(val_df.prompt.values)\n",
    "val_df_chosen = list(val_df.chosen.values)\n",
    "val_df_rejected = list(val_df.rejected.values)\n",
    "test_df_prompts = list(test_df.prompt.values)\n",
    "test_df_chosen = list(test_df.chosen.values)\n",
    "test_df_rejected = list(test_df.rejected.values)\n",
    "\n",
    "# log probs data per reference model\n",
    "log_prob_ref_dict = {}\n",
    "for SPLIT in [\"train\", \"val\", \"test\"]:\n",
    "    log_prob_ref_dict[SPLIT] = {}\n",
    "    for REFERENCE_MODEL in REFERENCE_MODELS:\n",
    "        df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/precomputed/{REFERENCE_MODEL}_{SPLIT}.csv\")\n",
    "        df.reset_index(inplace=True)\n",
    "        log_prob_ref_dict[SPLIT][REFERENCE_MODEL] = df\n",
    "\n",
    "# load in base model + tokenizer + peft + bitsandbytes\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ") if use_4bit else None\n",
    "target_modules = \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
    "lora_cfg = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=target_modules.split(\",\"),\n",
    ") if use_lora else None\n",
    "train_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    device_map=DEVICE,\n",
    "    quantization_config=bnb_cfg,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, use_fast=False)\n",
    "train_model = prepare_model_for_kbit_training(train_model)\n",
    "train_model = get_peft_model(train_model, lora_cfg)\n",
    "train_model.train()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "train_model.config.use_cache = False\n",
    "train_model.gradient_checkpointing_enable()\n",
    "train_model.enable_input_require_grads()\n",
    "train_model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(train_model.parameters(), lr=LR)\n",
    "\n",
    "alphas2use = None\n",
    "if ALPHA_METHOD == 'offline_1':\n",
    "    ref2alpha = {}\n",
    "    for ref_model, log_probs in log_prob_ref_dict['val'].items():\n",
    "        logprob_chosen = log_probs['logprob_chosen'].values\n",
    "        length_chosen = log_probs['L_chosen'].values\n",
    "        logprob_rejected = log_probs['logprob_rejected'].values\n",
    "        length_rejected = log_probs['L_rejected'].values\n",
    "        alpha = np.sum(np.abs(logprob_chosen/length_chosen - logprob_rejected/length_rejected))\n",
    "        ref2alpha[ref_model] = alpha\n",
    "    total_alpha = sum(ref2alpha.values())\n",
    "    for ref_model in ref2alpha.keys():\n",
    "        ref2alpha[ref_model] = ref2alpha[ref_model] / total_alpha\n",
    "    alphas_offline_1 = torch.tensor([ref2alpha[x] for x in REFERENCE_MODELS]).to(DEVICE)\n",
    "    alphas2use = alphas_offline_1[None,:]\n",
    "elif ALPHA_METHOD == 'offline_2':\n",
    "    ref2alpha = {}\n",
    "    for ref_model, log_probs in log_prob_ref_dict['val'].items():\n",
    "        logprob_chosen = log_probs['logprob_chosen'].values\n",
    "        length_chosen = log_probs['L_chosen'].values\n",
    "        logprob_rejected = log_probs['logprob_rejected'].values\n",
    "        length_rejected = log_probs['L_rejected'].values\n",
    "        alpha = np.sum((logprob_chosen/length_chosen > logprob_rejected/length_rejected))\n",
    "        ref2alpha[ref_model] = alpha\n",
    "    total_alpha = sum(ref2alpha.values())\n",
    "    for ref_model in ref2alpha.keys():\n",
    "        ref2alpha[ref_model] = ref2alpha[ref_model] / total_alpha\n",
    "    alphas_offline_2 = torch.tensor([ref2alpha[x] for x in REFERENCE_MODELS]).to(DEVICE)\n",
    "    alphas2use = alphas_offline_2[None,:]\n",
    "\n",
    "\n",
    "# training loop over epochs and batches\n",
    "# 1. Need to feed log_probs normalized\n",
    "logging_results = {'epoch':[], 'batch':[], 'alphas':[], 'batch_acc_pre':[], 'batch_acc_post':[], 'val_acc':[], 'test_acc':[], 'loss': [], 'dpo_by_refs':[]}\n",
    "alphas2use_from_prev_batch = torch.ones((1, len(REFERENCE_MODELS)), device=DEVICE) / len(REFERENCE_MODELS)\n",
    "\n",
    "run_config = {\n",
    "    \"dataset\": DATASET,\n",
    "    \"seed\": SEED,\n",
    "    \"use_mrpo_over_mdpo\": USE_MRPO_OVER_MDPO,\n",
    "    \"alpha_method\": ALPHA_METHOD,\n",
    "    \"base_model\": BASE_MODEL_PATH,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"lr\": LR,\n",
    "    \"beta\": BETA,\n",
    "    \"use_length_normalization\": use_length_normalization,\n",
    "    \"use_4bit\": use_4bit,\n",
    "    \"use_lora\": use_lora,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc='epoch'):\n",
    "    for batch in tqdm(range(NUM_BATCHES), desc='batch'):\n",
    "        global_step = epoch * NUM_BATCHES + batch\n",
    "\n",
    "        logging_results['epoch'].append(epoch)\n",
    "        logging_results['batch'].append(batch)\n",
    "\n",
    "        batch_data = train_df.loc[batch * BATCH_SIZE : ((batch+1) * BATCH_SIZE) - 1]\n",
    "        batch_prompt, batch_chosen, batch_rejected = batch_data[\"prompt\"].tolist(), batch_data[\"chosen\"].tolist(), batch_data[\"rejected\"].tolist()\n",
    "\n",
    "        def stack_column(col_name):\n",
    "            arrs = [\n",
    "                torch.tensor(\n",
    "                    data_model.loc[batch_data.index][col_name].values,\n",
    "                    dtype=torch.float16      # cast once\n",
    "                )\n",
    "                for _, data_model in log_prob_ref_dict[\"train\"].items()\n",
    "            ]\n",
    "            return torch.stack(arrs, dim=0).T.to(DEVICE)\n",
    "        preferred_ref_log_probs = stack_column(\"logprob_chosen\")\n",
    "        nonpreferred_ref_log_probs = stack_column(\"logprob_rejected\")\n",
    "        L_chosen_refs           = stack_column(\"L_chosen\")\n",
    "        L_rejected_refs         = stack_column(\"L_rejected\")\n",
    "\n",
    "        # compute accuracy on this batch BEFORE we do the gradient update!\n",
    "        with torch.no_grad():\n",
    "            batch_acc_pre = compute_acc(model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=200, device=DEVICE)\n",
    "            logging_results['batch_acc_pre'].append(batch_acc_pre.detach().cpu().numpy())\n",
    "\n",
    "        # compute alphas to use for this batch unless offline\n",
    "        if ALPHA_METHOD == 'arwc_original':\n",
    "            alphas = torch.abs(preferred_ref_log_probs-nonpreferred_ref_log_probs)\n",
    "            alphas2use = alphas / torch.sum(alphas, dim=1, keepdim=True) # B x K\n",
    "        elif ALPHA_METHOD == 'arwc_normalized':\n",
    "            alphas = torch.abs(preferred_ref_log_probs/L_chosen_refs-nonpreferred_ref_log_probs/L_rejected_refs)\n",
    "            alphas2use = alphas / torch.sum(alphas, dim=1, keepdim=True) # B x K\n",
    "        elif ALPHA_METHOD == 'online_1': # computed on previous batch\n",
    "            alphas2use = alphas2use_from_prev_batch # B x K\n",
    "        logging_results['alphas'].append(alphas2use.mean(0).detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        # compute the current log-probs on the training batch's preferred.\n",
    "        preferred_train_log_probs, L_chosen = CLRL(\n",
    "            model=train_model, tok=tok, prompts=batch_prompt, replies=batch_chosen, device=DEVICE)\n",
    "\n",
    "        # compute the current log-probs on the training batch's rejected.\n",
    "        nonpreferred_train_log_probs, L_rejected = CLRL(\n",
    "            model=train_model, tok=tok, prompts=batch_prompt, replies=batch_rejected, device=DEVICE)\n",
    "\n",
    "        # B tensors\n",
    "        preferred_train_log_probs = torch.stack(preferred_train_log_probs)\n",
    "        nonpreferred_train_log_probs = torch.stack(nonpreferred_train_log_probs)\n",
    "\n",
    "\n",
    "        # use length normalization\n",
    "\n",
    "        if use_length_normalization:\n",
    "            L_chosen = torch.from_numpy(np.asarray(L_chosen)).to(DEVICE)\n",
    "            L_rejected = torch.from_numpy(np.asarray(L_rejected)).to(DEVICE)\n",
    "            preferred_train_log_probs_2use = preferred_train_log_probs / L_chosen\n",
    "            nonpreferred_train_log_probs_2use = nonpreferred_train_log_probs / L_rejected\n",
    "            preferred_ref_log_probs_2use = preferred_ref_log_probs / L_chosen_refs\n",
    "            nonpreferred_ref_log_probs_2use = nonpreferred_ref_log_probs / L_rejected_refs\n",
    "\n",
    "        # forward-pass\n",
    "\n",
    "\n",
    "        loss = compute_objective(\n",
    "            preferred_train_log_probs_2use,\n",
    "            nonpreferred_train_log_probs_2use,\n",
    "            preferred_ref_log_probs_2use,\n",
    "            nonpreferred_ref_log_probs_2use,\n",
    "            beta=BETA, alphas=alphas2use)\n",
    "\n",
    "        # backward-pass\n",
    "        loss.backward()\n",
    "        logging_results['loss'].append(loss.item())\n",
    "        print(f\"Epoch {epoch}, Batch {batch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "        # update our parameters + zero our gradient\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute accuracy on this batch after training\n",
    "            batch_acc_post = compute_acc(model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=200, device=DEVICE)\n",
    "            logging_results['batch_acc_post'].append(batch_acc_post.detach().cpu().numpy())\n",
    "\n",
    "            # compute mdpo by ref logs\n",
    "            mdpo_losses = compute_dpo_objective_many_refs(\n",
    "                preferred_train_log_probs_2use,\n",
    "                nonpreferred_train_log_probs_2use,\n",
    "                preferred_ref_log_probs_2use,\n",
    "                nonpreferred_ref_log_probs_2use,\n",
    "                beta=BETA) # K\n",
    "            logging_results['dpo_by_refs'].append(mdpo_losses.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        # compute alphas for online 1 this batch will be used next batch\n",
    "        if ALPHA_METHOD == 'online_1': # computed on previous batch\n",
    "            alphas = torch.abs(preferred_ref_log_probs/L_chosen_refs-nonpreferred_ref_log_probs/L_rejected_refs)\n",
    "            alphas2use_from_prev_batch = alphas / torch.sum(alphas, dim=1, keepdim=True) # B x K\n",
    "\n",
    "        if (((batch+1)% (NUM_BATCHES//10)==0) or (batch == (NUM_BATCHES-1))):\n",
    "            with torch.no_grad():\n",
    "                # val acc\n",
    "                val_acc = compute_acc(model=train_model, tok=tok, prompt=val_df_prompts, chosen=val_df_chosen, rejected=val_df_rejected, MAX_BATCH=100, device=DEVICE)\n",
    "                logging_results['val_acc'].append(val_acc)\n",
    "                # test acc\n",
    "                test_acc = compute_acc(model=train_model, tok=tok, prompt=test_df_prompts, chosen=test_df_chosen, rejected=test_df_rejected, MAX_BATCH=100, device=DEVICE)\n",
    "                logging_results['test_acc'].append(test_acc)\n",
    "\n",
    "        else:\n",
    "            val_acc, test_acc = None, None\n",
    "            logging_results['val_acc'].append(None)\n",
    "            logging_results['test_acc'].append(None)\n",
    "\n",
    "        if (((batch+1)% (NUM_BATCHES//10)==0) or (batch == (NUM_BATCHES-1))):\n",
    "            df = expand_vector_columns(logging_results, REFERENCE_MODELS)\n",
    "            fname = LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MRPO.csv\" if USE_MRPO_OVER_MDPO else LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MDPO.csv\"\n",
    "            df.to_csv(fname, index=False)\n",
    "\n",
    "\n",
    "df = expand_vector_columns(logging_results, REFERENCE_MODELS)\n",
    "fname = LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MRPO.csv\" if USE_MRPO_OVER_MDPO else LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MDPO.csv\"\n",
    "df.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLvEZt7RTOrp"
   },
   "source": [
    "# ONLY FOR MICRO BATCHING IF ABOVE DOESN'T WORK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgMdkzZzNrVX"
   },
   "outputs": [],
   "source": [
    "use_micro_batch = False\n",
    "if use_micro_batch :\n",
    "    DATASET = \"ultrafeedback_binarized\"\n",
    "    SEED = 2\n",
    "    USE_MRPO_OVER_MDPO = True\n",
    "    ALPHA_METHOD = \"online_1\"\n",
    "    DEVICE = 'cuda:0'\n",
    "\n",
    "    assert ALPHA_METHOD in ALPHA_METHODS, f\"ALPHA_METHOD {ALPHA_METHOD} not in {ALPHA_METHODS}\"\n",
    "    assert DATASET in PRECOMPUTED_ALIASES.keys(), f\"DATASET {DATASET} not in {list(PRECOMPUTED_ALIASES.keys())}\"\n",
    "    assert SEED in SEEDS, f\"SEED {SEED} not in {SEEDS}\"\n",
    "\n",
    "    # check that MODEL_DIR has 7 folders each named by REFERENCE_MODELS\n",
    "    entries = [d for d in os.listdir(MODEL_DIR) if os.path.isdir(os.path.join(MODEL_DIR, d))]\n",
    "    missing = set(REFERENCE_MODELS) - set(entries)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing : {missing}\")\n",
    "\n",
    "    # These values are fixed\n",
    "    use_4bit = True\n",
    "    use_lora = True\n",
    "    LR, BETA, NUM_EPOCHS = 1e-4, 0.1, 1\n",
    "    BATCH_SIZE = 50 if DATASET == \"PKU-SafeRLHF-30K-standard\" else 25\n",
    "    # compute how many batches we will need\n",
    "    NUM_BATCHES = (5000 // BATCH_SIZE) if (5000 % BATCH_SIZE) == 0 else (5000 // BATCH_SIZE) + 1\n",
    "    use_length_normalization = True\n",
    "    compute_objective = compute_mrpo_objective if USE_MRPO_OVER_MDPO else compute_mdpo_objective\n",
    "\n",
    "\n",
    "\n",
    "    MICRO_BATCH = 5\n",
    "    assert BATCH_SIZE % MICRO_BATCH == 0\n",
    "    NUM_MICRO_BATCHES = BATCH_SIZE // MICRO_BATCH\n",
    "\n",
    "    # preference data (prompt, chosen, rejected)\n",
    "    train_df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/data/train.csv\")\n",
    "    val_df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/data/val.csv\")\n",
    "    test_df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/data/test.csv\")\n",
    "    val_df_prompts = list(val_df.prompt.values)\n",
    "    val_df_chosen = list(val_df.chosen.values)\n",
    "    val_df_rejected = list(val_df.rejected.values)\n",
    "    test_df_prompts = list(test_df.prompt.values)\n",
    "    test_df_chosen = list(test_df.chosen.values)\n",
    "    test_df_rejected = list(test_df.rejected.values)\n",
    "\n",
    "    # log probs data per reference model\n",
    "    log_prob_ref_dict = {}\n",
    "    for SPLIT in [\"train\", \"val\", \"test\"]:\n",
    "        log_prob_ref_dict[SPLIT] = {}\n",
    "        for REFERENCE_MODEL in REFERENCE_MODELS:\n",
    "            df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/precomputed/{REFERENCE_MODEL}_{SPLIT}.csv\")\n",
    "            df.reset_index(inplace=True)\n",
    "            log_prob_ref_dict[SPLIT][REFERENCE_MODEL] = df\n",
    "\n",
    "    # load in base model + tokenizer + peft + bitsandbytes\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    ) if use_4bit else None\n",
    "    target_modules = \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        target_modules=target_modules.split(\",\"),\n",
    "    ) if use_lora else None\n",
    "    train_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        device_map=DEVICE,\n",
    "        quantization_config=bnb_cfg,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tok = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, use_fast=True)\n",
    "    train_model = prepare_model_for_kbit_training(train_model)\n",
    "    train_model = get_peft_model(train_model, lora_cfg)\n",
    "    train_model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    train_model.config.use_cache = False\n",
    "    train_model.gradient_checkpointing_enable()\n",
    "    train_model.enable_input_require_grads()\n",
    "    train_model.to(DEVICE)\n",
    "\n",
    "    optimizer = AdamW(train_model.parameters(), lr=LR)\n",
    "\n",
    "    alphas2use = None\n",
    "    if ALPHA_METHOD == 'offline_1':\n",
    "        ref2alpha = {}\n",
    "        for ref_model, log_probs in log_prob_ref_dict['val'].items():\n",
    "            logprob_chosen = log_probs['logprob_chosen'].values\n",
    "            length_chosen = log_probs['L_chosen'].values\n",
    "            logprob_rejected = log_probs['logprob_rejected'].values\n",
    "            length_rejected = log_probs['L_rejected'].values\n",
    "            alpha = np.sum(np.abs(logprob_chosen/length_chosen - logprob_rejected/length_rejected))\n",
    "            ref2alpha[ref_model] = alpha\n",
    "        total_alpha = sum(ref2alpha.values())\n",
    "        for ref_model in ref2alpha.keys():\n",
    "            ref2alpha[ref_model] = ref2alpha[ref_model] / total_alpha\n",
    "        alphas_offline_1 = torch.tensor([ref2alpha[x] for x in REFERENCE_MODELS]).to(DEVICE)\n",
    "        alphas2use = alphas_offline_1[None,:]\n",
    "    elif ALPHA_METHOD == 'offline_2':\n",
    "        ref2alpha = {}\n",
    "        for ref_model, log_probs in log_prob_ref_dict['val'].items():\n",
    "            logprob_chosen = log_probs['logprob_chosen'].values\n",
    "            length_chosen = log_probs['L_chosen'].values\n",
    "            logprob_rejected = log_probs['logprob_rejected'].values\n",
    "            length_rejected = log_probs['L_rejected'].values\n",
    "            alpha = np.sum((logprob_chosen/length_chosen > logprob_rejected/length_rejected))\n",
    "            ref2alpha[ref_model] = alpha\n",
    "        total_alpha = sum(ref2alpha.values())\n",
    "        for ref_model in ref2alpha.keys():\n",
    "            ref2alpha[ref_model] = ref2alpha[ref_model] / total_alpha\n",
    "        alphas_offline_2 = torch.tensor([ref2alpha[x] for x in REFERENCE_MODELS]).to(DEVICE)\n",
    "        alphas2use = alphas_offline_2[None,:]\n",
    "\n",
    "\n",
    "    # training loop over epochs and batches\n",
    "    # 1. Need to feed log_probs normalized\n",
    "    logging_results = {'epoch':[], 'batch':[], 'alphas':[], 'batch_acc_pre':[], 'batch_acc_post':[], 'val_acc':[], 'test_acc':[], 'loss': [], 'dpo_by_refs':[]}\n",
    "    alphas2use_from_prev_batch = torch.ones((1, len(REFERENCE_MODELS)), device=DEVICE) / len(REFERENCE_MODELS)\n",
    "\n",
    "    run_config = {\n",
    "        \"dataset\": DATASET,\n",
    "        \"seed\": SEED,\n",
    "        \"use_mrpo_over_mdpo\": USE_MRPO_OVER_MDPO,\n",
    "        \"alpha_method\": ALPHA_METHOD,\n",
    "        \"base_model\": BASE_MODEL_PATH,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": LR,\n",
    "        \"beta\": BETA,\n",
    "        \"use_length_normalization\": use_length_normalization,\n",
    "        \"use_4bit\": use_4bit,\n",
    "        \"use_lora\": use_lora,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "    }\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(NUM_EPOCHS), desc='epoch'):\n",
    "        for batch in tqdm(range(NUM_BATCHES), desc='batch'):\n",
    "            global_step = epoch * NUM_BATCHES + batch\n",
    "\n",
    "            logging_results['epoch'].append(epoch)\n",
    "            logging_results['batch'].append(batch)\n",
    "\n",
    "            batch_data = train_df.loc[batch * BATCH_SIZE : ((batch+1) * BATCH_SIZE) - 1]\n",
    "            batch_prompt, batch_chosen, batch_rejected = batch_data[\"prompt\"].tolist(), batch_data[\"chosen\"].tolist(), batch_data[\"rejected\"].tolist()\n",
    "\n",
    "            def stack_column(col_name):\n",
    "                arrs = [\n",
    "                    torch.tensor(\n",
    "                        data_model.loc[batch_data.index][col_name].values,\n",
    "                        dtype=torch.float16      # cast once\n",
    "                    )\n",
    "                    for _, data_model in log_prob_ref_dict[\"train\"].items()\n",
    "                ]\n",
    "                return torch.stack(arrs, dim=0).T.to(DEVICE)\n",
    "            preferred_ref_log_probs = stack_column(\"logprob_chosen\")\n",
    "            nonpreferred_ref_log_probs = stack_column(\"logprob_rejected\")\n",
    "            L_chosen_refs           = stack_column(\"L_chosen\")\n",
    "            L_rejected_refs         = stack_column(\"L_rejected\")\n",
    "\n",
    "            # compute accuracy on this batch BEFORE we do the gradient update!\n",
    "            with torch.no_grad():\n",
    "                batch_acc_pre = compute_acc(model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=200, device=DEVICE)\n",
    "                logging_results['batch_acc_pre'].append(batch_acc_pre.detach().cpu().numpy())\n",
    "\n",
    "            # compute alphas to use for this batch unless offline\n",
    "            if ALPHA_METHOD == 'arwc_original':\n",
    "                alphas = torch.abs(preferred_ref_log_probs-nonpreferred_ref_log_probs)\n",
    "                alphas2use = alphas / torch.sum(alphas, dim=1, keepdim=True) # B x K\n",
    "            elif ALPHA_METHOD == 'arwc_normalized':\n",
    "                alphas = torch.abs(preferred_ref_log_probs/L_chosen_refs-nonpreferred_ref_log_probs/L_rejected_refs)\n",
    "                alphas2use = alphas / torch.sum(alphas, dim=1, keepdim=True) # B x K\n",
    "            elif ALPHA_METHOD == 'online_1': # computed on previous batch\n",
    "                alphas2use = alphas2use_from_prev_batch # B x K\n",
    "            logging_results['alphas'].append(alphas2use.mean(0).detach().cpu().numpy())\n",
    "\n",
    "            micro_losses = []\n",
    "            dpo_by_refs_accum = []\n",
    "            if alphas2use.shape[0] == 1:\n",
    "              alphas2use = alphas2use.repeat(BATCH_SIZE, 1)\n",
    "            for micro_batch in tqdm(range(NUM_MICRO_BATCHES), desc='micro_batch'):\n",
    "              micro_batch_prompt = batch_prompt[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "              micro_batch_chosen = batch_chosen[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "              micro_batch_rejected = batch_rejected[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "              micro_alphas2use = alphas2use[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "              micro_preferred_ref_log_probs = preferred_ref_log_probs[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "              micro_nonpreferred_ref_log_probs = nonpreferred_ref_log_probs[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "              micro_L_chosen_refs = L_chosen_refs[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "              micro_L_rejected_refs = L_rejected_refs[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "\n",
    "              # compute the current log-probs on the training batch's preferred.\n",
    "              micro_preferred_train_log_probs, micro_L_chosen = CLRL(\n",
    "                  model=train_model, tok=tok, prompts=micro_batch_prompt, replies=micro_batch_chosen, device=DEVICE)\n",
    "\n",
    "              # compute the current log-probs on the training batch's rejected.\n",
    "              micro_nonpreferred_train_log_probs, micro_L_rejected = CLRL(\n",
    "                  model=train_model, tok=tok, prompts=micro_batch_prompt, replies=micro_batch_rejected, device=DEVICE)\n",
    "\n",
    "              # B tensors\n",
    "              micro_preferred_train_log_probs = torch.stack(micro_preferred_train_log_probs)\n",
    "              micro_nonpreferred_train_log_probs = torch.stack(micro_nonpreferred_train_log_probs)\n",
    "\n",
    "              # use length normalization\n",
    "\n",
    "              micro_L_chosen = torch.from_numpy(np.asarray(micro_L_chosen)).to(DEVICE)\n",
    "              micro_L_rejected = torch.from_numpy(np.asarray(micro_L_rejected)).to(DEVICE)\n",
    "              micro_preferred_train_log_probs_2use = micro_preferred_train_log_probs / micro_L_chosen\n",
    "              micro_nonpreferred_train_log_probs_2use = micro_nonpreferred_train_log_probs / micro_L_rejected\n",
    "              micro_preferred_ref_log_probs_2use = micro_preferred_ref_log_probs / micro_L_chosen_refs\n",
    "              micro_nonpreferred_ref_log_probs_2use = micro_nonpreferred_ref_log_probs / micro_L_rejected_refs\n",
    "\n",
    "              # forward-pass\n",
    "              loss = compute_objective(\n",
    "                  micro_preferred_train_log_probs_2use,\n",
    "                  micro_nonpreferred_train_log_probs_2use,\n",
    "                  micro_preferred_ref_log_probs_2use,\n",
    "                  micro_nonpreferred_ref_log_probs_2use,\n",
    "                  beta=BETA, alphas=micro_alphas2use) / NUM_MICRO_BATCHES\n",
    "\n",
    "              # backward-pass\n",
    "              loss.backward()\n",
    "              micro_losses.append(loss.item())\n",
    "              print(f\"Epoch {epoch}, Batch {batch}, micro_batch {micro_batch}, Loss: {loss.item()}\")\n",
    "              with torch.no_grad():\n",
    "                mb_dpo_losses = compute_dpo_objective_many_refs(\n",
    "                    micro_preferred_train_log_probs_2use,   # (m)\n",
    "                    micro_nonpreferred_train_log_probs_2use,\n",
    "                    micro_preferred_ref_log_probs_2use,\n",
    "                    micro_nonpreferred_ref_log_probs_2use,\n",
    "                    beta=BETA\n",
    "                )\n",
    "                dpo_by_refs_accum.append(mb_dpo_losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # update our parameters + zero our gradient\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            logging_results['loss'].append(np.sum(micro_losses))\n",
    "\n",
    "            dpo_by_refs_full = torch.vstack(dpo_by_refs_accum)   # (B, K)\n",
    "            logging_results[\"dpo_by_refs\"].append(\n",
    "                dpo_by_refs_full.mean(0).detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # compute accuracy on this batch after training\n",
    "                batch_acc_post = compute_acc(model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=200, device=DEVICE)\n",
    "                logging_results['batch_acc_post'].append(batch_acc_post.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "            # compute alphas for online 1 this batch will be used next batch\n",
    "            if ALPHA_METHOD == 'online_1': # computed on previous batch\n",
    "                alphas = torch.abs(preferred_ref_log_probs/L_chosen_refs-nonpreferred_ref_log_probs/L_rejected_refs)\n",
    "                alphas2use_from_prev_batch = alphas / torch.sum(alphas, dim=1, keepdim=True) # B x K\n",
    "\n",
    "            if (((batch+1)% (NUM_BATCHES//10)==0) or (batch == (NUM_BATCHES-1))):\n",
    "                with torch.no_grad():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.ipc_collect()\n",
    "                    gc.collect()\n",
    "                    # val acc\n",
    "                    val_acc = compute_acc(model=train_model, tok=tok, prompt=val_df_prompts, chosen=val_df_chosen, rejected=val_df_rejected, MAX_BATCH=50, device=DEVICE)\n",
    "                    logging_results['val_acc'].append(val_acc)\n",
    "                    # test acc\n",
    "                    test_acc = compute_acc(model=train_model, tok=tok, prompt=test_df_prompts, chosen=test_df_chosen, rejected=test_df_rejected, MAX_BATCH=50, device=DEVICE)\n",
    "                    logging_results['test_acc'].append(test_acc)\n",
    "\n",
    "            else:\n",
    "                val_acc, test_acc = None, None\n",
    "                logging_results['val_acc'].append(None)\n",
    "                logging_results['test_acc'].append(None)\n",
    "\n",
    "            if (((batch+1)% (NUM_BATCHES//10)==0) or (batch == (NUM_BATCHES-1))):\n",
    "                df = expand_vector_columns(logging_results, REFERENCE_MODELS)\n",
    "                fname = LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MRPO.csv\" if USE_MRPO_OVER_MDPO else LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MDPO.csv\"\n",
    "                df.to_csv(fname, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = expand_vector_columns(logging_results, REFERENCE_MODELS)\n",
    "    fname = LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MRPO.csv\" if USE_MRPO_OVER_MDPO else LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MDPO.csv\"\n",
    "    df.to_csv(fname, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
